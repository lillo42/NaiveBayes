{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import hashlib\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    #__tamanhoBroaCast= 0\n",
    "    __treinoRDD = None\n",
    "    __PARDD  = None\n",
    "    __dictPx = {}\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def Treino(self, treinoRDD):\n",
    "        self.__treinoRDD = treinoRDD\n",
    "        #self.__tamanhoBroaCast =  sc.broadcast(treinoRDD.count())\n",
    "        self.__PreCalculaPA(treinoRDD)\n",
    "        self.__CalculaPx(treinoRDD)\n",
    "        return\n",
    "    \n",
    "    def __PreCalculaPA(self,treinoRDD):\n",
    "        N = treinoRDD.count()\n",
    "        self.__PARDD = (treinoRDD\n",
    "                        .flatMap(lambda x: [((y,x.features[y]),1) for y in x.features.indices])\n",
    "                        .reduceByKey(lambda x,y:x+y)\n",
    "                        .map(lambda x:(x[0],math.log(x[1]/float(N))))\n",
    "                          )\n",
    "        return\n",
    "    \n",
    "    def __CalculaPx(self,treinoRDD):\n",
    "        N = treinoRDD.count()\n",
    "        self.__dictPx = (treinoRDD\n",
    "                                .map(lambda x: (x.label,1))\n",
    "                                .reduceByKey(lambda x,y:x+y)\n",
    "                                .map(lambda x: (x[0], math.log(x[1]/float(N))))\n",
    "                                .collectAsMap()\n",
    "                           )\n",
    "        return\n",
    "    \n",
    "    def Predicao(self, objetoDesconhecido, mostraValores = False):\n",
    "        maxClassificao = (-1,-100000000000000)\n",
    "        for key in self.__dictPx.keys():\n",
    "                PAix = self.__CalculaPAix(objetoDesconhecido,key,self.__treinoRDD)\n",
    "                PAi = self.__CalculaPAi(objetoDesconhecido,self.__PARDD)\n",
    "                Px = self.__dictPx[key]\n",
    "                PxAi = PAix + Px - PAi\n",
    "                \n",
    "                if mostraValores:\n",
    "                    print 'Key:' + str(key)\n",
    "                    print '\\t Px: ' + str(Px)\n",
    "                    print '\\t PAix: ' + str(PAix)\n",
    "                    print '\\t PAi: ' + str(PAi)\n",
    "                    print '\\t Conta(Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): (Log(' + str(Px) + ') + Log('+ str(PAix) + ') - Log(' + str(PxAi) + ')'\n",
    "                    print '\\t Resultado: (Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): ' + str(PxAi)\n",
    "                if maxClassificao[1] < PxAi:\n",
    "                    maxClassificao = (key,PxAi)\n",
    "                    \n",
    "        return maxClassificao[0]\n",
    "    \n",
    "    def __CalculaPAix(self, objetoDesconhecido, key,treinoRDD):\n",
    "        retorno = 1\n",
    "        N = treinoRDD.count()\n",
    "        treinoFiltroRDD = treinoRDD.filter(lambda x: x.label == key)\n",
    "        quantidadeTreioFiltro = treinoFiltroRDD.count()        \n",
    "        \n",
    "        for i in (objetoDesconhecido.features.indices):            \n",
    "            quantidadePorFeature = (treinoFiltroRDD\n",
    "                                     .filter(lambda x: i in x.features.indices)\n",
    "                                     .count()\n",
    "                                   )\n",
    "            retorno += math.log((1 + quantidadePorFeature)/float(quantidadeTreioFiltro + N))\n",
    "        return retorno\n",
    "    \n",
    "    def __CalculaPAi(self, objetoDesconhecido,PARDD):\n",
    "        retorno = 1\n",
    "        for i in (objetoDesconhecido.features.indices):\n",
    "            retorno += (PARDD\n",
    "                        .filter(lambda x: x[0][0] == i and x[0][1] == objetoDesconhecido.features[i])\n",
    "                        .take(1)[0][1]\n",
    "                       )\n",
    "        return retorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parsePoint(point):\n",
    "    \"\"\"Converts a comma separated string into a list of (featureID, value) tuples.\n",
    "\n",
    "    Note:\n",
    "        featureIDs should start at 0 and increase to the number of features - 1.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of (featureID, value) tuples.\n",
    "    \"\"\"\n",
    "    pPoint = point.split(',')[1:]\n",
    "    return [(pId, pValue) for (pId, pValue) in enumerate(pPoint)]\n",
    "\n",
    "def createOneHotDict(inputData):\n",
    "    \"\"\"Creates a one-hot-encoder dictionary based on the input data.\n",
    "\n",
    "    Args:\n",
    "        inputData (RDD of lists of (int, str)): An RDD of observations where each observation is\n",
    "            made up of a list of (featureID, value) tuples.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the keys are (featureID, value) tuples and map to values that are\n",
    "            unique integers.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (inputData\n",
    "            .flatMap(lambda sd: sd)\n",
    "            .distinct()\n",
    "            .zipWithIndex()\n",
    "            .collectAsMap()\n",
    "           )\n",
    "\n",
    "def oneHotEncoding(rawFeats, OHEDict, numOHEFeats):\n",
    "    \"\"\"Produce a one-hot-encoding from a list of features and an OHE dictionary.\n",
    "\n",
    "    Note:\n",
    "        If a (featureID, value) tuple doesn't have a corresponding key in OHEDict it should be\n",
    "        ignored.\n",
    "\n",
    "    Args:\n",
    "        rawFeats (list of (int, str)): The features corresponding to a single observation.  Each\n",
    "            feature consists of a tuple of featureID and the feature's value. (e.g. sampleOne)\n",
    "        OHEDict (dict): A mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The total number of unique OHE features (combinations of featureID and\n",
    "            value).LabelAndFeatures\n",
    "\n",
    "    Returns:\n",
    "        SparseVector: A SparseVector of length numOHEFeats with indicies equal to the unique\n",
    "            identifiers for the (featureID, value) combinations that occur in the observation and\n",
    "            with values equal to 1.0.\n",
    "    \"\"\"\n",
    "    return SparseVector(numOHEFeats,[(OHEDict[x],1) for x in rawFeats if OHEDict.has_key(x)])\n",
    "\n",
    "def parseOHEPoint(point, OHEDict, numOHEFeats):\n",
    "    \"\"\"Obtain the label and feature vector for this raw observation.\n",
    "\n",
    "    Note:\n",
    "        You must use the function `oneHotEncoding` in this implementation or later portions\n",
    "        of this lab may not function as expected.\n",
    "\n",
    "    Args:\n",
    "        point (str): A comma separated string where the first value is the label and the rest\n",
    "            are features.\n",
    "        OHEDict (dict of (int, str) to int): Mapping of (featureID, value) to unique integer.\n",
    "        numOHEFeats (int): The number of unique features in the training dataset.\n",
    "\n",
    "    Returns:\n",
    "        LabeledPoint: Contains the label for the observation and the one-hot-encoding of the\n",
    "            raw features based on the provided OHE dictionary.\n",
    "    \"\"\"\n",
    "    pPoint = point.split(',')\n",
    "    return LabeledPoint(pPoint[0], oneHotEncoding(parsePoint(point), OHEDict, numOHEFeats))\n",
    "\n",
    "def hashFunction(numBuckets, rawFeats, printMapping=False):\n",
    "    \"\"\"Calculate a feature dictionary for an observation's features based on hashing.\n",
    "\n",
    "    Note:\n",
    "        Use printMapping=True for debug purposes and to better understand how the hashing works.\n",
    "\n",
    "    Args:\n",
    "        numBuckets (int): Number of buckets to use as features.\n",
    "        rawFeats (list of (int, str)): A list of features for an observation.  Represented as\n",
    "            (featureID, value) tuples.\n",
    "        printMapping (bool, optional): If true, the mappings of featureString to index will be\n",
    "            printed.\n",
    "\n",
    "    Returns:\n",
    "        dict of int to float:  The keys will be integers which represent the buckets that the\n",
    "            features have been hashed to.  The value for a given key will contain the count of the\n",
    "            (featureID, value) tuples that have hashed to that key.\n",
    "    \"\"\"\n",
    "    mapping = {}\n",
    "    for featureString in rawFeats.split():\n",
    "        mapping[featureString] = int(int(hashlib.md5(featureString).hexdigest(), 16) % numBuckets)\n",
    "    if(printMapping): print mapping\n",
    "    sparseFeatures = defaultdict(float)\n",
    "    for bucket in mapping.values():\n",
    "        sparseFeatures[bucket] += 1.0\n",
    "    return dict(sparseFeatures)\n",
    "\n",
    "def parseTextPoint(point, numBuckets):\n",
    "    #id0, id1, id2, text, sent = point.split('\\t') #Exemplo Professor\n",
    "    id1, id2, text, sent = point.split('\\t') #Exemplo Professor\n",
    "    sent = int(sent)\n",
    "    if sent < 2:\n",
    "        label = 0\n",
    "    elif sent > 2:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 2\n",
    "    print text\n",
    "    features = SparseVector(numBuckets,hashFunction(numBuckets, text))\n",
    "    \n",
    "    return LabeledPoint(label,features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classificando objeto 1:\n",
      "Key:0.0\n",
      "\t Px: -0.441832752279\n",
      "\t PAix: -2.12593782921\n",
      "\t PAi: -1.79320800944\n",
      "\t Conta(Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): (Log(-0.441832752279) + Log(-2.12593782921) - Log(-0.774562572045)\n",
      "\t Resultado: (Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): -0.774562572045\n",
      "Key:1.0\n",
      "\t Px: -1.02961941718\n",
      "\t PAix: -1.56655063883\n",
      "\t PAi: -1.79320800944\n",
      "\t Conta(Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): (Log(-1.02961941718) + Log(-1.56655063883) - Log(-0.802962046567)\n",
      "\t Resultado: (Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): -0.802962046567\n",
      "Classificando objeto 2:\n",
      "Key:0.0\n",
      "\t Px: -0.441832752279\n",
      "\t PAix: -2.12593782921\n",
      "\t PAi: -1.97552956624\n",
      "\t Conta(Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): (Log(-0.441832752279) + Log(-2.12593782921) - Log(-0.592241015251)\n",
      "\t Resultado: (Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): -0.592241015251\n",
      "Key:1.0\n",
      "\t Px: -1.02961941718\n",
      "\t PAix: -2.54737989184\n",
      "\t PAi: -1.97552956624\n",
      "\t Conta(Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): (Log(-1.02961941718) + Log(-2.54737989184) - Log(-1.60146974278)\n",
      "\t Resultado: (Log(P(Ai|x))+ Log(P(x)) - Log(P(Ai))): -1.60146974278\n",
      "Resultado1: seguro\n",
      "Resultado2: seguro\n"
     ]
    }
   ],
   "source": [
    "#Seguro 0\n",
    "#Perigo 1\n",
    "dictTmp = {}\n",
    "dictTmp[0] = \"seguro\"\n",
    "dictTmp[1] = \"Perigo\"\n",
    "\n",
    "listaTreino = []\n",
    "listaTreino.append(\"0,peludo,marrom,grande,dura\")\n",
    "listaTreino.append(\"0,peludo,verde,grande,dura\")\n",
    "listaTreino.append(\"1,liso,vermelho,grande,macia\")\n",
    "listaTreino.append(\"0,peludo,verde,grande,macia\")\n",
    "listaTreino.append(\"0,peludo,vermelho,pequeno,dura\")\n",
    "listaTreino.append(\"0,liso,vermelho,pequeno,dura\")\n",
    "listaTreino.append(\"0,liso,marrom,pequeno,dura\")\n",
    "listaTreino.append(\"1,peludo,verde,pequeno,macia\")\n",
    "listaTreino.append(\"1,liso,verde,pequeno,dura\")\n",
    "listaTreino.append(\"0,peludo,vermelho,grande,dura\")\n",
    "listaTreino.append(\"0,liso,marrom,grande,macia\")\n",
    "listaTreino.append(\"1,liso,verde,pequeno,macia\")\n",
    "listaTreino.append(\"0,peludo,vermelho,pequeno,macia\")\n",
    "listaTreino.append(\"1,liso,vermelho,grande,dura\")\n",
    "\n",
    "sampleDataRDD = sc.parallelize(listaTreino)\n",
    "ctrOHEDict = createOneHotDict(sampleDataRDD.map(parsePoint))\n",
    "numCtrOHEFeats = len(ctrOHEDict.keys())\n",
    "OHETrainData = sampleDataRDD.map(lambda point: parseOHEPoint(point, ctrOHEDict, numCtrOHEFeats))\n",
    "\n",
    "nb = NaiveBayes()\n",
    "nb.Treino(OHETrainData)\n",
    "\n",
    "objetoDesconhecido1 = (\"3,liso,vermelho,pequeno,dura\")\n",
    "lpObjetoDesconhecido1 =  parseOHEPoint(objetoDesconhecido1, ctrOHEDict, numCtrOHEFeats)\n",
    "\n",
    "objetoDesconhecido2 = (\"3,peludo,verde,pequeno,dura\")\n",
    "lpObjetoDesconhecido2 = parseOHEPoint(objetoDesconhecido2, ctrOHEDict, numCtrOHEFeats)\n",
    "\n",
    "print \"Classificando objeto 1:\"\n",
    "retorno1 = nb.Predicao(lpObjetoDesconhecido1,True)\n",
    "\n",
    "print \"Classificando objeto 2:\"\n",
    "retorno2 = nb.Predicao(lpObjetoDesconhecido2,True)\n",
    "\n",
    "print \"Resultado1: \" + dictTmp[retorno1]\n",
    "print \"Resultado2: \" + dictTmp[retorno2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhraseId\tSentenceId\tPhrase\tSentiment\n",
      "[u'1\\t1\\tA series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .\\t1']\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(\"Data\",\"Aula04\",\"MovieReviews.tsv\")\n",
    "#filename = os.path.join(\"Data\",\"Aula04\",\"Test.tsv\")\n",
    "rawRDD = sc.textFile(filename,2)\n",
    "header = rawRDD.take(1)[0]\n",
    "\n",
    "dataRDD = rawRDD.filter(lambda x: x!=header)\n",
    "\n",
    "weights = [.8, .1, .1]\n",
    "seed = 42\n",
    "rawTrainData, rawValidationData, rawTestData = dataRDD.randomSplit(weights, seed)\n",
    "# Cache the data\n",
    "rawTrainData.cache()\n",
    "rawValidationData.cache()\n",
    "rawTestData.cache()\n",
    "\n",
    "'''\n",
    "https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/\n",
    "\n",
    "The sentiment labels are:\n",
    "\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "'''\n",
    "\n",
    "print header\n",
    "print dataRDD.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LabeledPoint(0.0, (5000,[96,122,143,479,664,1138,1224,1351,1425,1497,1635,1648,1793,2084,2405,2539,2920,3214,3672,3804,3849,3876,4057,4345,4380,4675,4870,4993],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))]\n"
     ]
    }
   ],
   "source": [
    "numBuckets = 5000\n",
    "parsedTrainData = rawTrainData.map(lambda x: parseTextPoint(x, numBuckets)).cache()\n",
    "parsedValData = rawValidationData.map(lambda x: parseTextPoint(x, numBuckets)).cache()\n",
    "parsedTestData = rawTestData.map(lambda x: parseTextPoint(x, numBuckets)).cache()\n",
    "\n",
    "binTrainData = parsedTrainData.filter(lambda x: x.label!=2).cache()\n",
    "binValData = parsedValData.filter(lambda x: x.label!=2).cache()\n",
    "binTestData = parsedTestData.filter(lambda x: x.label!=2).cache()\n",
    "\n",
    "print parsedTrainData.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred  0.0\n",
      "pred  0.0\n",
      "pred  0.0\n",
      "pred  1.0\n",
      "pred  1.0\n",
      "pred  1.0\n",
      "pred  1.0\n",
      "pred  1.0\n",
      "pred  1.0\n",
      "pred  0.0\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes()\n",
    "trainDataNB = nb.Treino(binTrainData)\n",
    "objet = binValData.take(10)\n",
    "#print objet[0].features\n",
    "for i in objet:\n",
    "    pred = nb.Predicao(i)\n",
    "    print 'pred ', pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
